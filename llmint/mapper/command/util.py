import yaml
import os

class pcolors:
    RIGHT = '\033[92m'
    WRONG = '\033[91m'
    MISSING = '\033[33m'
    ENDC = '\033[0m'

# Read OpenAI key from ~/.llmint/config.yaml
def get_openai_api_key():
    """ 
    Read OpenAI key from ~/.llmint/config.yaml

    Returns:
        string: OpenAI key
    """
    with open(os.path.expanduser("~/.llmint/config.yaml"), "r") as f:
        config = yaml.load(f, Loader=yaml.SafeLoader)
        if "openai_api_key" not in config:
            raise Exception("OpenAI API key not found in ~/.llmint/config.yaml,"
                            " please add 'openai_api_key: YOUR_KEY' in the file.")
        openai_api_key = config["openai_api_key"]
        return openai_api_key


def from_yaml(filepath):
    """ 
    Load yaml file.

    Args:
        filepath (string): absolute file path of yaml file

    Returns:
        dict or string: parsed YAML file
    """
    with open(filepath, 'r') as f:
        return yaml.load(f, Loader=yaml.SafeLoader)
    
def format_source_target(source, target):
    return "Source Schema: " + source + "\nTarget Schema: " + target

def get_system_prompt(messages):
    """ 
    Append all system prompts to messages.
    
    Args:
        messages (list): list of messages input to model
        
    Returns:
        list: messages with appended system prompts
    """
    module_dir = os.path.dirname(__file__)
    
    # base instructional message
    with open(os.path.join(module_dir, 'prompt', 'llmint_base.txt')) as f:
        messages.append({"role": "system",
                    "content": f.read()
                })
    # STL instructional message
    with open(os.path.join(module_dir, 'prompt', 'stl_base.txt')) as f:
        messages.append({"role": "system",
                        "content": f.read()
                        })
    # end instructional message
    with open(os.path.join(module_dir, 'prompt', 'end_base.txt')) as f:
        messages.append({"role": "system",
                        "content": f.read()
                        })
    return messages 

def get_user_prompt(messages, source, target):
    """
    Format source and target input and append to messages.

    Args:
        messages (list): list of messages input to model
        source (string): source schema provided by user
        target (string): target schema provided by user
    
    Returns:
        list: messages with appended system prompts
    """
    messages.append({
        "role": "user",
        "content": format_source_target(source, target)
    })
    
    return messages

# accuracy measured by # of correct mappings / total mappings
def accuracy(results, example_num, example_mappings):
    """
    Compares model's results with ground truth mappings and prints recall/precision 
    measurements for accuracy. The model's results are printed in color, where green
    means correct, red means the model's generated mapping is incorrect, yellow means
    the ground truth mapping was not identified by the model.

    Args:
        results (list): list of mappings generated by the model
        example_num (int): index of which ground truth mapping in example_mappings
                           should be compared against
        example_mappings (list): list of ground truth mappings
    """
    correct = False
    correctIdxs = []
    numCorrect = 0
    total = 0
    print("Generated Mappings:", flush=True)
    for result, reasoning in results:
        for i in range(len(example_mappings[example_num]["mapping"])):
            if result == str(example_mappings[example_num]["mapping"][i]).replace("'", ""):
                print(pcolors.RIGHT + result + pcolors.ENDC + '\n', reasoning, flush=True)
                numCorrect += 1
                correctIdxs.append(i)
                correct = True 
        if not correct: 
            print(pcolors.WRONG + result + pcolors.ENDC + '\n', reasoning, flush=True)
        correct = False
        total += 1
    print("Ground Truth Mappings:", flush=True)
    for i in range(len(example_mappings[example_num]["mapping"])):
        if i in correctIdxs:
            print(pcolors.RIGHT + str(example_mappings[example_num]["mapping"][i]).replace("'", "") + pcolors.ENDC, flush=True)
        else:
            print(pcolors.MISSING + str(example_mappings[example_num]["mapping"][i]).replace("'", "") + pcolors.ENDC, flush=True)    
    
    precision = len(correctIdxs) / len(results)       
    recall = len(correctIdxs) / len(example_mappings[example_num]["mapping"])
    f1 = 2 * ((precision * recall) / (precision + recall))
    
    print("Precision: ", precision, flush=True)
    print("Recall: ", recall, flush=True)
    print("F1: ", f1, flush=True)
    print("Total: ", numCorrect, "/", total, flush=True)
    return precision, recall, f1
        
def print_responses(response, include_reasoning):
    """
    Print all responses.

    Args:
        response (list): list of responses to print
        include_reasoning (bool): whether or not to print model's reasoning in the output
    """
    for result, reasoning in response:
        if include_reasoning:
            print(pcolors.RIGHT + result + pcolors.ENDC + '\n', reasoning, flush=True)
        else:
            print(pcolors.RIGHT + result + pcolors.ENDC, flush=True)